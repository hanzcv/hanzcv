<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Home - Hanz Cuevas Velasquez</title>
    <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abhaya+Libre">
    <link rel="stylesheet" href="assets/fonts/font-awesome.min.css">
    <link rel="stylesheet" href="assets/css/untitled-1.css">
    <link rel="stylesheet" href="assets/css/untitled.css">
</head>

<body id="page-top">
    <nav class="navbar navbar-light navbar-expand-lg fixed-top bg-secondary text-uppercase" id="mainNav">
        <div class="container"><a class="navbar-brand js-scroll-trigger" href="#page-top">Hanz Cuevas Velasquez</a><button data-toggle="collapse" data-target="#navbarResponsive" class="navbar-toggler navbar-toggler-right text-uppercase bg-primary text-white rounded" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><i class="fa fa-bars"></i></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#portfolio">work</a></li>
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#about">Short CV</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <header class="masthead bg-primary text-white text-center" style="background-position: center;background-image: url(&quot;assets/img/uni_1.jpeg&quot;);">
        <div class="container"><img class="img-fluid d-block mx-auto mb-5" style="width: 309px;" src="assets/img/yo_2.png">
            <h1 style="font-size: 51px;">Hanz Cuevas Velasquez</h1>
            <h2 class="font-weight-light mb-0" style="font-size: 24px;">Ph.D. 3D Compter Vision</h2>
            <hr>
            <h2 class="font-weight-light mb-0" style="font-size: 19px;">BC-1.12, Bayes Centre, School of Informatics&nbsp;</h2>
            <h2 class="font-weight-light mb-0" style="font-size: 19px;">University of Edinburgh</h2>
            <h2 class="font-weight-light mb-0" style="font-size: 19px;">47 Potterrow, Edinburgh EH8 9BT</h2>
            <h2 class="font-weight-light mb-0" style="font-size: 19px;"></h2>
        </div>
    </header>
    <section id="portfolio" class="portfolio">
        <div class="container">
            <h2 class="text-uppercase text-center text-secondary">work</h2>
            <hr>
            <div class="col">
                <div class="row">
                    <div class="col-md-6 col-lg-4"><a class="d-block mx-auto portfolio-item" data-toggle="modal" href="#portfolio-modal-novelhead">
                            <div class="d-flex portfolio-item-caption position-absolute h-100 w-100">
                                <div class="my-auto portfolio-item-caption-content w-100 text-center text-white"><i class="fa fa-search-plus fa-3x"></i></div>
                            </div><img class="img-fluid d-lg-flex justify-content-center align-items-center justify-content-lg-center align-items-lg-center" src="assets/img/out_eye.gif" style="width: 50%;margin: 0 auto;" width="56" height="100">
                        </a></div>
                    <div class="col">
                        <div class="row">
                            <div class="col">
                                <h1 style="font-size: 20px;">Novel Head Pose, Expression, and Gaze Synthesis</h1>
                            </div>
                        </div>
                        <h1 class="text-justify" style="font-size: 14px;font-family: Lato, sans-serif;"><strong>Current work</strong></h1>
                        <div class="row">
                            <div class="col">
                                <h1 class="text-justify" style="font-size: 17px;font-family: Lato, sans-serif;">Our method enables the synthesis of novel head poses, expressions, and gazes from short cellphone videos. It allows for explicit control of the eye and jaw positions, as well as the rendering of new poses and expressions.</h1>
                                <h1 class="text-justify" style="font-size: 14px;font-family: Lato, sans-serif;"><br></h1>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <hr>
                    </div>
                </div>
                <div class="row">
                    <div class="col-md-6 col-lg-4"><a class="d-block mx-auto portfolio-item" data-toggle="modal" href="#portfolio-proc-humans">
                            <div class="d-flex portfolio-item-caption position-absolute h-100 w-100">
                                <div class="my-auto portfolio-item-caption-content w-100 text-center text-white"><i class="fa fa-search-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/procedural_humans.jpg">
                        </a></div>
                    <div class="col">
                        <div class="row">
                            <div class="col">
                                <h1 style="font-size: 20px;">Procedural Humans for Computer Vision<br></h1>
                            </div>
                        </div>
                        <h1 class="text-justify" style="font-size: 14px;font-family: Lato, sans-serif;"><strong>Microsoft report</strong><br></h1>
                        <div class="row">
                            <div class="col">
                                <h1 class="text-justify" style="font-size: 17px;font-family: Lato, sans-serif;">Part of my work at Microsoft. A report where we describe how we construct a parametric model of the face and body, including articulated hands; our rendering pipeline to generate realistic images of humans based on this body model; and a method for fitting our body model to dense landmarks predicted from multiple views.<br><a href="https://www.microsoft.com/en-us/research/publication/procedural-humans-for-computer-vision/">Project page</a></h1>
                                <h1 class="text-justify" style="font-size: 14px;font-family: Lato, sans-serif;"><br></h1>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <hr>
                    </div>
                </div>
                <div class="row">
                    <div class="col-md-6 col-lg-4"><a class="d-block mx-auto portfolio-item" data-toggle="modal" href="#portfolio-modal-gelatto">
                            <div class="d-flex portfolio-item-caption position-absolute h-100 w-100">
                                <div class="my-auto portfolio-item-caption-content w-100 text-center text-white"><i class="fa fa-search-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/gelatto.png">
                        </a></div>
                    <div class="col">
                        <div class="row">
                            <div class="col">
                                <h1 style="font-size: 20px;"><strong>Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Segmentation</strong></h1>
                            </div>
                        </div>
                        <h1 class="text-justify" style="font-size: 14px;font-family: Lato, sans-serif;"><strong>BMVC 2021</strong><br></h1>
                        <div class="row">
                            <div class="col">
                                <h1 class="text-justify" style="font-size: 17px;font-family: Lato, sans-serif;"><strong>An innovative two-headed attention layer that combines geometric and latent features to segment a 3D scene into semantically meaningful subsets. Each head combines local and global information, using either the geometric or latent features, of a neighborhood of points and uses this information to learn better local relationships.</strong><br><a href="https://www.microsoft.com/en-us/research/publication/procedural-humans-for-computer-vision/">Paper</a></h1>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <hr>
                    </div>
                </div>
                <div class="row">
                    <div class="col-md-6 col-lg-4"><a class="d-block mx-auto portfolio-item" data-toggle="modal" href="#portfolio-modal-trimbot">
                            <div class="d-flex portfolio-item-caption position-absolute h-100 w-100">
                                <div class="my-auto portfolio-item-caption-content w-100 text-center text-white"><i class="fa fa-search-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/robot.jpg">
                        </a></div>
                    <div class="col">
                        <div class="row">
                            <div class="col">
                                <h1 style="font-size: 20px;"><strong>Real-time Stereo Visual Servoing for Rose Pruning with Robotic Arm</strong><br></h1>
                            </div>
                        </div>
                        <h1 class="text-justify" style="font-size: 14px;font-family: Lato, sans-serif;"><strong>ICRA 2020</strong><br></h1>
                        <div class="row">
                            <div class="col">
                                <h1 class="text-justify" style="font-size: 17px;font-family: Lato, sans-serif;"><strong>Visual servoing to find and cut rose branches in real-time in a real garden using a robotic arm and eye-in-hand stereo camera.&nbsp;</strong><br><a href="https://www.microsoft.com/en-us/research/publication/procedural-humans-for-computer-vision/">Paper</a></h1>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <hr>
                    </div>
                </div>
                <div class="row">
                    <div class="col-md-6 col-lg-4"><a class="d-block mx-auto portfolio-item" data-toggle="modal" href="#portfolio-modal-plant">
                            <div class="d-flex portfolio-item-caption position-absolute h-100 w-100">
                                <div class="my-auto portfolio-item-caption-content w-100 text-center text-white"><i class="fa fa-search-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/4_4_outputpng.png">
                        </a></div>
                    <div class="col">
                        <div class="row">
                            <div class="col">
                                <h1 style="font-size: 20px;"><strong>Segmentation and 3D Reconstruction of Rose Plants from Stereoscopic Images</strong><br></h1>
                            </div>
                        </div>
                        <h1 class="text-justify" style="font-size: 14px;font-family: Lato, sans-serif;"><strong>Computers and Electronics in Agriculture 2020</strong><br></h1>
                        <div class="row">
                            <div class="col">
                                <h1 class="text-justify" style="font-size: 17px;font-family: Lato, sans-serif;"><strong>Part of the vision module of a garden robot capable of navigating towards rose bushes and clip them according to a set of pruning rules. The method is responsible for performing the segmentation of the branches and recovering their morphology in 3D.&nbsp;</strong><br><a href="https://www.microsoft.com/en-us/research/publication/procedural-humans-for-computer-vision/">Paper</a></h1>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <hr>
                    </div>
                </div>
                <div class="row">
                    <div class="col-md-6 col-lg-4"><a class="d-block mx-auto portfolio-item" data-toggle="modal" href="#portfolio-modal-multiview">
                            <div class="d-flex portfolio-item-caption position-absolute h-100 w-100">
                                <div class="my-auto portfolio-item-caption-content w-100 text-center text-white"><i class="fa fa-search-plus fa-3x"></i></div>
                            </div><img class="img-fluid" src="assets/img/hybrid.png">
                        </a></div>
                    <div class="col">
                        <div class="row">
                            <div class="col">
                                <h1 style="font-size: 20px;"><strong>Hybrid Multi-camera Visual Servoing to Moving Target</strong></h1>
                            </div>
                        </div>
                        <h1 class="text-justify" style="font-size: 14px;font-family: Lato, sans-serif;"><strong>IROS 2018</strong><br></h1>
                        <div class="row">
                            <div class="col">
                                <h1 class="text-justify" style="font-size: 17px;font-family: Lato, sans-serif;"><strong>A novel hybrid multi-camera eye-to-hand (EtoH) / eye-in-hand (EinH) approach to guide a robotarm in different tasks. The target point is assumed to bedynamic, which makes the problem more complex in termsof the switching between EtoH and EinH servoing as thespatial relationship between the robot and target changes.</strong><br><a href="https://www.microsoft.com/en-us/research/publication/procedural-humans-for-computer-vision/">Paper</a></h1>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section id="about" class="bg-primary text-white mb-0" style="background: rgb(24,188,156);">
        <div class="container">
            <h2 class="text-uppercase text-center text-white">Short CV</h2>
            <hr>
            <div class="col">
                <div class="row">
                    <div class="col">
                        <p class="lead">- (2023 - Now) Visiting researcher</p>
                        <p class="lead" style="font-size: 15px;margin-top: -21px;">&nbsp; &nbsp;The University of Edinburgh</p>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <p class="lead">- (2022 - 2022) Researcher intern</p>
                        <p class="lead" style="font-size: 15px;margin-top: -21px;">&nbsp; &nbsp; Microsoft</p>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <p class="lead">- (2017 - 2022) Ph.D. in Informatics - 3D Computer Vision</p>
                        <p class="lead" style="font-size: 15px;margin-top: -21px;">&nbsp; &nbsp;The University of Edinburgh</p>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <p class="lead">- (2017- 2016) MSc. Artificial Intelligence&nbsp;</p>
                        <p class="lead" style="font-size: 15px;margin-top: -21px;">&nbsp; &nbsp;The University of Edinburgh</p>
                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <p class="lead">- (2011- 2015) BSc. Mechatronics Engineering</p>
                        <p class="lead" style="font-size: 15px;margin-top: -21px;">&nbsp; &nbsp;Universidad Catolica Boliviana "San Pablo"</p>
                    </div>
                </div>
            </div>
            <div class="text-center mt-4"></div>
        </div>
    </section>
    <div class="copyright py-4 text-center text-white">
        <div class="container"></div>
    </div>
    <div class="d-lg-none scroll-to-top position-fixed rounded"><a class="d-block js-scroll-trigger text-center text-white rounded" href="#page-top"><i class="fa fa-chevron-up"></i></a></div>
    <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-modal-novelhead">
        <div class="modal-dialog modal-lg" role="document">
            <div class="modal-content">
                <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                <div class="modal-body">
                    <div class="container text-center">
                        <div class="row">
                            <div class="col-lg-8 mx-auto">
                                <h2 class="text-uppercase text-secondary mb-0"><strong><span style="color: rgb(33, 37, 41);">Novel Head Pose, Expression, and Gaze Synthesis</span></strong></h2>
                                <hr>
                                <div class="embed-responsive embed-responsive-16by9"><iframe class="embed-responsive-item" src="https://www.youtube.com/embed/bxBUUKn_ZsU?ab_channel=HanzCuevas" allowfullscreen="" id="player"></iframe></div>
                                <p class="text-justify mb-5"><strong>Our method enables the synthesis of novel head poses, expressions, and gazes from short cellphone videos. It allows for explicit control of the eye and jaw positions, as well as the rendering of new poses and expressions.</strong></p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="modal-footer pb-5"></div>
            </div>
        </div>
    </div>
    <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-proc-humans">
        <div class="modal-dialog modal-lg" role="document">
            <div class="modal-content">
                <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                <div class="modal-body">
                    <div class="container text-center">
                        <div class="row">
                            <div class="col-lg-8 mx-auto">
                                <h2 class="text-uppercase text-secondary mb-0">Procedural Humans for Computer Vision</h2><img id="proc-human-4" class="proc-human" src="assets/img/procedural_humans-2.png">
                                <hr>
                                <p class="text-justify mb-5">Recent work has shown the benefits of synthetic data for use in computer vision, with applications ranging from autonomous driving to face landmark detection and reconstruction. There are a number of benefits of using synthetic data from privacy preservation and bias elimination to quality and feasibility of annotation. Generating human-centered synthetic data is a particular challenge in terms of realism and domain-gap, though recent work has shown that effective machine learning models can be trained using synthetic face data alone. We show that this can be extended to include the full body by building on the pipeline of&nbsp;Wood et al. to generate synthetic images of humans in their entirety, with ground-truth annotations for computer vision applications.<br><br>In this report we describe how we construct a parametric model of the face and body, including articulated hands; our rendering pipeline to generate realistic images of humans based on this body model; an approach for training DNNs to regress a dense set of landmarks covering the entire body; and a method for fitting our body model to dense landmarks predicted from multiple views.&nbsp;<br></p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="modal-footer pb-5"></div>
                <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-proc-humans-5">
                    <div class="modal-dialog modal-lg" role="document">
                        <div class="modal-content">
                            <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                            <div class="modal-body">
                                <div class="container text-center">
                                    <div class="row">
                                        <div class="col-lg-8 mx-auto">
                                            <h2 class="text-uppercase text-secondary mb-0">Procedural Humans for Computer Vision</h2><img id="proc-human-5" class="proc-human" src="assets/img/procedural_humans-2.png">
                                            <hr>
                                            <p class="text-justify mb-5">Recent work has shown the benefits of synthetic data for use in computer vision, with applications ranging from autonomous driving to face landmark detection and reconstruction. There are a number of benefits of using synthetic data from privacy preservation and bias elimination to quality and feasibility of annotation. Generating human-centered synthetic data is a particular challenge in terms of realism and domain-gap, though recent work has shown that effective machine learning models can be trained using synthetic face data alone. We show that this can be extended to include the full body by building on the pipeline of&nbsp;Wood et al. to generate synthetic images of humans in their entirety, with ground-truth annotations for computer vision applications.<br><br>In this report we describe how we construct a parametric model of the face and body, including articulated hands; our rendering pipeline to generate realistic images of humans based on this body model; an approach for training DNNs to regress a dense set of landmarks covering the entire body; and a method for fitting our body model to dense landmarks predicted from multiple views.&nbsp;<br></p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="modal-footer pb-5"></div>
                        </div>
                    </div>
                </div>
                <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-proc-humans-6">
                    <div class="modal-dialog modal-lg" role="document">
                        <div class="modal-content">
                            <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                            <div class="modal-body">
                                <div class="container text-center">
                                    <div class="row">
                                        <div class="col-lg-8 mx-auto">
                                            <h2 class="text-uppercase text-secondary mb-0">Procedural Humans for Computer Vision</h2><img id="proc-human-6" class="proc-human" src="assets/img/procedural_humans-2.png">
                                            <hr>
                                            <p class="text-justify mb-5">Recent work has shown the benefits of synthetic data for use in computer vision, with applications ranging from autonomous driving to face landmark detection and reconstruction. There are a number of benefits of using synthetic data from privacy preservation and bias elimination to quality and feasibility of annotation. Generating human-centered synthetic data is a particular challenge in terms of realism and domain-gap, though recent work has shown that effective machine learning models can be trained using synthetic face data alone. We show that this can be extended to include the full body by building on the pipeline of&nbsp;Wood et al. to generate synthetic images of humans in their entirety, with ground-truth annotations for computer vision applications.<br><br>In this report we describe how we construct a parametric model of the face and body, including articulated hands; our rendering pipeline to generate realistic images of humans based on this body model; an approach for training DNNs to regress a dense set of landmarks covering the entire body; and a method for fitting our body model to dense landmarks predicted from multiple views.&nbsp;<br></p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="modal-footer pb-5"></div>
                            <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-proc-humans-7">
                                <div class="modal-dialog modal-lg" role="document">
                                    <div class="modal-content">
                                        <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                                        <div class="modal-body">
                                            <div class="container text-center">
                                                <div class="row">
                                                    <div class="col-lg-8 mx-auto">
                                                        <h2 class="text-uppercase text-secondary mb-0">Procedural Humans for Computer Vision</h2><img id="proc-human-7" class="proc-human" src="assets/img/procedural_humans-2.png">
                                                        <hr>
                                                        <p class="text-justify mb-5">Recent work has shown the benefits of synthetic data for use in computer vision, with applications ranging from autonomous driving to face landmark detection and reconstruction. There are a number of benefits of using synthetic data from privacy preservation and bias elimination to quality and feasibility of annotation. Generating human-centered synthetic data is a particular challenge in terms of realism and domain-gap, though recent work has shown that effective machine learning models can be trained using synthetic face data alone. We show that this can be extended to include the full body by building on the pipeline of&nbsp;Wood et al. to generate synthetic images of humans in their entirety, with ground-truth annotations for computer vision applications.<br><br>In this report we describe how we construct a parametric model of the face and body, including articulated hands; our rendering pipeline to generate realistic images of humans based on this body model; an approach for training DNNs to regress a dense set of landmarks covering the entire body; and a method for fitting our body model to dense landmarks predicted from multiple views.&nbsp;<br></p>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="modal-footer pb-5"></div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-modal-gelatto">
        <div class="modal-dialog modal-lg" role="document">
            <div class="modal-content">
                <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                <div class="modal-body">
                    <div class="container text-center">
                        <div class="row">
                            <div class="col-lg-8 mx-auto">
                                <h2 class="text-uppercase text-secondary mb-0">Ge-Latto</h2>
                                <hr>
                                <div class="embed-responsive embed-responsive-16by9"><iframe class="embed-responsive-item" src="https://www.youtube.com/embed/mjsttn3C89g" allowfullscreen="" id="player"></iframe></div>
                                <p class="text-justify mb-5">Ge-Latto is a two-headed local attention layer that evaluates a patch <br>inside the point cloud and tries to find good relationships between the neighbor points.</p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="modal-footer pb-5"></div>
            </div>
        </div>
    </div>
    <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-modal-multiview">
        <div class="modal-dialog modal-lg" role="document">
            <div class="modal-content">
                <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                <div class="modal-body">
                    <div class="container text-center">
                        <div class="row">
                            <div class="col-lg-8 mx-auto">
                                <h2 class="text-uppercase text-secondary mb-0"><strong>Hybrid Multi-camera Visual Servoing to Moving Target</strong><br></h2>
                                <hr>
                                <div class="embed-responsive embed-responsive-16by9"><iframe class="embed-responsive-item" src="https://www.youtube.com/embed/OEiZu0gaP6w" allowfullscreen="" id="player"></iframe></div>
                                <p class="text-justify mb-5">A novel hybrid multi-camera eye-to-hand (EtoH) / eye-in-hand (EinH) approach to guide a robotarm in different tasks. The target point is assumed to bedynamic, which makes the problem more complex in termsof the switching between EtoH and EinH servoing as thespatial relationship between the robot and target changes.</p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="modal-footer pb-5"></div>
            </div>
        </div>
    </div>
    <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-modal-trimbot">
        <div class="modal-dialog modal-lg" role="document">
            <div class="modal-content">
                <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                <div class="modal-body">
                    <div class="container text-center">
                        <div class="row">
                            <div class="col-lg-8 mx-auto">
                                <h2 class="text-uppercase text-secondary mb-0"><strong>Real-time Stereo Visual Servoing</strong></h2>
                                <hr>
                                <div class="embed-responsive embed-responsive-16by9"><iframe class="embed-responsive-item" src="https://www.youtube.com/embed/r9IHy5lH8YM" allowfullscreen="" id="player"></iframe></div>
                                <p class="text-justify mb-5">Visual servoing to find and cut rose branches in real-time in a real garden using a robotic arm and eye-in-hand stereo camera. <br></p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="modal-footer pb-5"></div>
            </div>
        </div>
    </div>
    <div class="modal text-center" role="dialog" tabindex="-1" id="portfolio-modal-plant">
        <div class="modal-dialog modal-lg" role="document">
            <div class="modal-content">
                <div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div>
                <div class="modal-body">
                    <div class="container text-center">
                        <div class="row">
                            <div class="col-lg-8 mx-auto">
                                <h2 class="text-uppercase text-secondary mb-0"><strong>Segmentation and 3D Reconstruction of Rose Plants from Stereoscopic Images</strong><br></h2>
                                <hr>
                                <div class="embed-responsive embed-responsive-16by9"><iframe class="embed-responsive-item" src="https://www.youtube.com/embed/r9IHy5lH8YM" allowfullscreen="" id="player"></iframe></div>
                                <p class="text-justify mb-5">Part of the vision module of a garden robot capable of navigating towards rose bushes and clip them according to a set of pruning rules. The method is responsible for performing the segmentation of the branches and recovering their morphology in 3D. <br></p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="modal-footer pb-5"></div>
            </div>
        </div>
    </div>
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/bootstrap/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
    <script src="assets/js/freelancer.js"></script>
</body>

</html>